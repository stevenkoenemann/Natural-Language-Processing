{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will go through simple preprocessing and building a LSTM neural network. The neural network will be tasked with classifying the sentiment of imbd reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up EGPU for training using plaidml\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Librarys\n",
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import optimizers\n",
    "# Load dataset\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "print('Loading data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Print length of new datasets\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "# Pad data to equal lengths to feed into the model\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# Embedding layer converts words to integers\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "# LSTM layer with dropout to prevent overfitting\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# Final layer of model with sigmoid function \n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create otimizer\n",
    "o=optimizers.adam(lr=0.0001)\n",
    "\n",
    "# Compile Model \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=o,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2413 of 9426 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 7806 of 9426 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24992/25000 [============================>.] - ETA: 0s - loss: 0.5911 - acc: 0.7031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2246 of 9427 operations complete\n",
      "INFO:plaidml:Analyzing Ops: 7963 of 9427 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 590s 24ms/step - loss: 0.5911 - acc: 0.7031 - val_loss: 0.4357 - val_acc: 0.8135\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 567s 23ms/step - loss: 0.3967 - acc: 0.8420 - val_loss: 0.3667 - val_acc: 0.8382\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 568s 23ms/step - loss: 0.3160 - acc: 0.8756 - val_loss: 0.3560 - val_acc: 0.8421\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 567s 23ms/step - loss: 0.2669 - acc: 0.8993 - val_loss: 0.3739 - val_acc: 0.8360\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 568s 23ms/step - loss: 0.2336 - acc: 0.9133 - val_loss: 0.3806 - val_acc: 0.8364\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 568s 23ms/step - loss: 0.2069 - acc: 0.9258 - val_loss: 0.3863 - val_acc: 0.8332\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 567s 23ms/step - loss: 0.1860 - acc: 0.9337 - val_loss: 0.4441 - val_acc: 0.8266\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 567s 23ms/step - loss: 0.1647 - acc: 0.9437 - val_loss: 0.4278 - val_acc: 0.8216\n",
      "25000/25000 [==============================] - 250s 10ms/step\n",
      "Test score: 0.4278096935367584\n",
      "Test accuracy: 0.82164\n"
     ]
    }
   ],
   "source": [
    "# Train model with training dataset\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=8,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Test trained model with testing dataset\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "# Print results\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training of the model for eight epochs we can see that the model became better at predicting the seentiment in the train dataset but idd not perform the same in the test dataset. As we can see from looking at the validation accuracy, the model became worse at classifying sentiment after the third epoch decreasing from .8421 to .8216. This is not a huge decrease but it suggests that the model may be overfitting to the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
