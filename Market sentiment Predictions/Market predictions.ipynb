{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will attempt to predict weather returns of the S&P 500 will be positive or negative the month following a federal reserve meeting. We will do this by using the summary section of the beige book as our model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up connection to s3\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='stevenkoenemann-sagemaker' # Replace with your s3 bucket name\n",
    "prefix = 'sagemaker/xgboost-mnist' # Used as part of the path in the bucket where you store data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket) # The URL to access the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Minutes</th>\n",
       "      <th>Bond Sentiment</th>\n",
       "      <th>S&amp;P Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-30</td>\n",
       "      <td>Reports from the twelve Federal Reserve Distri...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-10-31</td>\n",
       "      <td>Reports from the twelve Federal Reserve Distri...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-08-31</td>\n",
       "      <td>Reports from the twelve Federal Reserve Distri...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-31</td>\n",
       "      <td>Overall economic activity increased at a modes...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>Reports from the twelve Federal Reserve Distri...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                            Minutes  \\\n",
       "0  2013-11-30  Reports from the twelve Federal Reserve Distri...   \n",
       "1  2013-10-31  Reports from the twelve Federal Reserve Distri...   \n",
       "2  2013-08-31  Reports from the twelve Federal Reserve Distri...   \n",
       "3  2013-07-31  Overall economic activity increased at a modes...   \n",
       "4  2013-06-30  Reports from the twelve Federal Reserve Distri...   \n",
       "\n",
       "   Bond Sentiment  S&P Sentiment  \n",
       "0               1              1  \n",
       "1               0              1  \n",
       "2               1              0  \n",
       "3               1              1  \n",
       "4               1              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve data from s3\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='stevenkoenemann-sagemaker'\n",
    "data_key = 'market_predictions.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "data = pd.read_csv(data_location)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in attempting this task is to clean and process the text data within the dataframe. To do this we will use NLTK and loop through the text in the dataframe to make the text lowercase, split the text into words and stem the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#cleaning the text\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0,168):\n",
    "    minutes = re.sub('[^a-zA-Z]', ' ', data['Minutes'][i])\n",
    "    minutes = minutes.lower()\n",
    "    minutes = minutes.split()\n",
    "    ps = PorterStemmer()\n",
    "    minutes = [ps.stem(word) for word in minutes if not word in set(stopwords.words('english'))]\n",
    "    minutes = ' '.join(minutes)\n",
    "    corpus.append(minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus\n",
    "\n",
    "y = data['S&P Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in processing text is to convert the text that we have cleaned and processed into numbers so that we have something to input inot the model. For this we will use the keras one_hot function. This function will integer encode our text using the hash_trick function and will not one hot encode the whole document. We will assign a vocab size to the function so that it knows how many number to assign. This number should be about the number of unique words in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 500\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we encode our document into numbers we must pad the sequences because they are most likely not all the same length. The default setting of this function will add zeros to the end of each sequence to make it as long as the max_length which we will assign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 500\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the text has been processed completely we will build our LSTM model using keras. LSTM is a type of recurrent neural network which means that the out put state from oen node is saved and used as an input for the next node along with the next input. This helps the network recognize patterns better in in ttext data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 18:28:13.300153 139939023046464 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0902 18:28:13.317251 139939023046464 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0902 18:28:13.319942 139939023046464 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0902 18:28:14.985152 139939023046464 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0902 18:28:14.992717 139939023046464 deprecation.py:506] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0902 18:28:16.037556 139939023046464 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0902 18:28:16.043812 139939023046464 deprecation_wrapper.py:119] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0902 18:28:16.048414 139939023046464 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "# Build model\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# Embedding layer converts words to integers\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_length))\n",
    "# LSTM layer with dropout to prevent overfitting\n",
    "model.add(LSTM(500, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(500, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(500, return_sequences=True))\n",
    "model.add(LSTM(200))\n",
    "# Final layer of model with sigmoid function \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Create otimizer\n",
    "o=optimizers.Nadam(lr=0.000001)\n",
    "\n",
    "# Compile Model \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=o,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/10\n",
      "168/168 [==============================] - 292s 2s/step - loss: 0.6930 - acc: 0.6012\n",
      "Epoch 2/10\n",
      "168/168 [==============================] - 282s 2s/step - loss: 0.6928 - acc: 0.6250\n",
      "Epoch 3/10\n",
      "168/168 [==============================] - 284s 2s/step - loss: 0.6927 - acc: 0.6369\n",
      "Epoch 4/10\n",
      "168/168 [==============================] - 288s 2s/step - loss: 0.6925 - acc: 0.6369\n",
      "Epoch 5/10\n",
      " 45/168 [=======>......................] - ETA: 3:28 - loss: 0.6925 - acc: 0.6222"
     ]
    }
   ],
   "source": [
    "# Train model with training dataset\n",
    "print('Train...')\n",
    "model.fit(padded_docs, y,\n",
    "          batch_size=5,\n",
    "          epochs=10)\n",
    "model.save_weights('fed_predictions.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this are not quite as good as I had hoped but there may be reason for this. The first reason that the model didnt perform that well may be that I simply did not have enough data. There are only 128 data points which is not that many for a mchine learning project like this. The second reason that this may not have worked is because I conflated sentiment of text with market return. The fed could talk positively about the economy and the market not respond accordingly due to other factors. Overall, I think that adding more data points and using more of the federal reserve minutes would improve the performance of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
